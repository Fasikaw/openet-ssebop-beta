{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import ee\n",
    "from IPython.display import Image\n",
    "\n",
    "import openet.ssebop as ssebop\n",
    "import utils\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2018-10-01'\n",
    "end_date = '2018-12-04'\n",
    "\n",
    "# start_date = '2015-07-01'\n",
    "# end_date = '2015-07-31'\n",
    "# start_date = '2016-07-01'\n",
    "# end_date = '2016-07-31'\n",
    "# start_date = '2017-02-07'\n",
    "# end_date = '2017-02-07'\n",
    "\n",
    "overwrite_flag = True\n",
    "\n",
    "min_pixel_count = 1000\n",
    "# min_scene_count = 10\n",
    "max_cloud_cover = 70\n",
    "# These don't do anything yet...\n",
    "# landsat5_flag = False\n",
    "# landsat7_flag = True\n",
    "# landsat8_flag = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tcorr Output Image Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcorr_img_name = 'topowx_median_v0_daily'\n",
    "tcorr_img_coll_id = 'projects/usgs-ssebop/tcorr_image/{}'.format(tcorr_img_name)\n",
    "tcorr_img_coll = ee.ImageCollection(tcorr_img_coll_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refreshing due to a 401 (attempt 1/2)\n",
      "Refreshing access_token\n"
     ]
    }
   ],
   "source": [
    "export_crs = 'EPSG:4326'\n",
    "export_geom = ee.Geometry.Rectangle([-125, 24, -65, 50], proj=export_crs, geodesic=False)  # CONUS\n",
    "# export_geom = ee.Geometry.Rectangle([-124, 35, -119, 42], proj=export_crs, geodesic=False)  # California\n",
    "export_region = export_geom.bounds(1, export_crs).coordinates().getInfo()[0][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tmax Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_source = 'topowx'\n",
    "tmax_version = 'median_v0'\n",
    "tmax_name = '{}_{}'.format(tmax_source.lower(), tmax_version.lower())\n",
    "tmax_coll_id = 'projects/usgs-ssebop/tmax/{}'.format(tmax_name)\n",
    "\n",
    "tmax_coll = ee.ImageCollection(tmax_coll_id)\n",
    "tmax_img = ee.Image(tmax_coll.first()).set('TMAX_VERSION', tmax_version.upper())\n",
    "\n",
    "# print(ee.Image(tmax_median_coll.first()).projection().getInfo()['transform'])\n",
    "# print(ee.Image(tmax_median_coll.first()).projection().getInfo()['crs'])\n",
    "# print(ee.Image(tmax_median_coll.first()).getInfo()['bands'][0]['dimensions'])\n",
    "tmax_geo = [0.00833333329998709, 0.0, -125.00416722008521, 0.0, -0.00833333329998709, 51.19583312184854]\n",
    "tmax_crs = 'EPSG:4326'\n",
    "tmax_shape = [7000, 3250]\n",
    "tmax_extent = [tmax_geo[2], tmax_geo[5] + tmax_shape[1] * tmax_geo[4], \n",
    "               tmax_geo[2] + tmax_shape[0] * tmax_geo[0], tmax_geo[5]]\n",
    "\n",
    "# Image(url=tmax_img.getThumbURL({'min': 270, 'max': 330, 'region': export_region}))\n",
    "# embed=True, format='png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Extent, Shape, Geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1749, 786]\n",
      "[0.03333333333333333, 0.0, -125.00416722008521, 0.0, -0.03333333333333333, 51.19583312184854]\n"
     ]
    }
   ],
   "source": [
    "# export_cs = 0.008333333333333333333333  # ~800m\n",
    "# export_cs = 0.016666666666666666666666  # ~1600m\n",
    "export_cs = 0.033333333333333333333333  # ~3200m\n",
    "export_crs = 'EPSG:4326'\n",
    "\n",
    "# Compute clipped Tmax grid (this is a disaster of code)\n",
    "export_xy = ee.Array(export_geom.bounds(1, export_crs).coordinates().get(0)).transpose().toList()\n",
    "export_xmin = ee.Number(ee.List(export_xy.get(0)).reduce(ee.Reducer.min()))\n",
    "export_ymin = ee.Number(ee.List(export_xy.get(1)).reduce(ee.Reducer.min()))\n",
    "export_xmax = ee.Number(ee.List(export_xy.get(0)).reduce(ee.Reducer.max()))\n",
    "export_ymax = ee.Number(ee.List(export_xy.get(1)).reduce(ee.Reducer.max()))\n",
    "# Snap to Tmax grid\n",
    "export_xmin = export_xmin.subtract(tmax_extent[0]).divide(export_cs).floor().multiply(export_cs).add(tmax_extent[0])\n",
    "export_ymin = export_ymin.subtract(tmax_extent[3]).divide(export_cs).floor().multiply(export_cs).add(tmax_extent[3])\n",
    "export_xmax = export_xmax.subtract(tmax_extent[0]).divide(export_cs).ceil().multiply(export_cs).add(tmax_extent[0])\n",
    "export_ymax = export_ymax.subtract(tmax_extent[3]).divide(export_cs).ceil().multiply(export_cs).add(tmax_extent[3])\n",
    "#  Limit to Tmax grid\n",
    "export_xmin = export_xmin.max(tmax_extent[0]).min(tmax_extent[2])\n",
    "export_ymin = export_ymin.max(tmax_extent[1]).min(tmax_extent[3])\n",
    "export_xmax = export_xmax.min(tmax_extent[0]).max(tmax_extent[2])\n",
    "export_ymax = export_ymax.min(tmax_extent[1]).max(tmax_extent[3])\n",
    "\n",
    "# export_extent = ee.List([export_xmin, export_ymin, export_xmax, export_ymax])\n",
    "export_geo = [export_cs, 0.0, export_xmin, 0.0, -export_cs, export_ymax])\n",
    "export_shape = [\n",
    "  export_xmax.subtract(export_xmin).abs().divide(export_cs).int(),\n",
    "  export_ymax.subtract(export_ymin).abs().divide(export_cs).int()]\n",
    "print(export_shape)\n",
    "print(export_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the Tcorr Image for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01\n",
      "  Asset already exists, removing\n",
      "2018-10-02\n",
      "  Asset already exists, removing\n",
      "2018-10-03\n",
      "  Asset already exists, removing\n",
      "2018-10-04\n",
      "  Asset already exists, removing\n",
      "2018-10-05\n",
      "  Asset already exists, removing\n",
      "2018-10-06\n",
      "  Asset already exists, removing\n",
      "2018-10-07\n",
      "  Asset already exists, removing\n",
      "2018-10-08\n",
      "  Asset already exists, removing\n",
      "2018-10-09\n",
      "  Asset already exists, removing\n",
      "2018-10-10\n",
      "  Asset already exists, removing\n",
      "2018-10-11\n",
      "  Asset already exists, removing\n",
      "2018-10-12\n",
      "  Asset already exists, removing\n",
      "2018-10-13\n",
      "  Asset already exists, removing\n",
      "2018-10-14\n",
      "  Asset already exists, removing\n",
      "2018-10-15\n",
      "  Asset already exists, removing\n",
      "2018-10-16\n",
      "  Asset already exists, removing\n",
      "2018-10-17\n",
      "  Asset already exists, removing\n",
      "2018-10-18\n",
      "  Asset already exists, removing\n",
      "2018-10-19\n",
      "  Asset already exists, removing\n",
      "2018-10-20\n",
      "  Asset already exists, removing\n",
      "2018-10-21\n",
      "  Asset already exists, removing\n",
      "2018-10-22\n",
      "  Asset already exists, removing\n",
      "2018-10-23\n",
      "  Asset already exists, removing\n",
      "2018-10-24\n",
      "  Asset already exists, removing\n",
      "2018-10-25\n",
      "  Asset already exists, removing\n",
      "2018-10-26\n",
      "  Asset already exists, removing\n",
      "2018-10-27\n",
      "  Asset already exists, removing\n",
      "2018-10-28\n",
      "  Asset already exists, removing\n",
      "2018-10-29\n",
      "  Asset already exists, removing\n",
      "2018-10-30\n",
      "  Asset already exists, removing\n",
      "2018-10-31\n",
      "  Asset already exists, removing\n",
      "2018-11-01\n",
      "  Asset already exists, removing\n",
      "2018-11-02\n",
      "  Asset already exists, removing\n",
      "2018-11-03\n",
      "  Asset already exists, removing\n",
      "2018-11-04\n",
      "  Asset already exists, removing\n",
      "2018-11-05\n",
      "  Asset already exists, removing\n",
      "2018-11-06\n",
      "  Asset already exists, removing\n",
      "2018-11-07\n",
      "  Asset already exists, removing\n",
      "2018-11-08\n",
      "  Asset already exists, removing\n",
      "2018-11-09\n",
      "  Asset already exists, removing\n",
      "2018-11-10\n",
      "  Asset already exists, removing\n",
      "2018-11-11\n",
      "  Asset already exists, removing\n",
      "2018-11-12\n",
      "  Asset already exists, removing\n",
      "2018-11-13\n",
      "  Asset already exists, removing\n",
      "2018-11-14\n",
      "  Asset already exists, removing\n",
      "2018-11-15\n",
      "  Asset already exists, removing\n",
      "2018-11-16\n",
      "  Asset already exists, removing\n",
      "2018-11-17\n",
      "  Asset already exists, removing\n",
      "2018-11-18\n",
      "  Asset already exists, removing\n",
      "2018-11-19\n",
      "  Asset already exists, removing\n",
      "2018-11-20\n",
      "  Asset already exists, removing\n",
      "2018-11-21\n",
      "  Asset already exists, removing\n",
      "2018-11-22\n",
      "  Asset already exists, removing\n",
      "2018-11-23\n",
      "  Asset already exists, removing\n",
      "2018-11-24\n",
      "  Asset already exists, removing\n",
      "2018-11-25\n",
      "  Asset already exists, removing\n",
      "2018-11-26\n",
      "  Asset already exists, removing\n",
      "2018-11-27\n",
      "  Asset already exists, removing\n",
      "2018-11-28\n",
      "  Asset already exists, removing\n",
      "2018-11-29\n",
      "  Asset already exists, removing\n",
      "2018-11-30\n",
      "  Asset already exists, removing\n",
      "2018-12-01\n",
      "  Asset already exists, removing\n",
      "2018-12-02\n",
      "2018-12-03\n",
      "2018-12-04\n"
     ]
    }
   ],
   "source": [
    "# Get current task list\n",
    "tasks = utils.get_ee_tasks()\n",
    "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
    "    logging.debug('  Tasks: {}'.format(len(tasks)))\n",
    "\n",
    "# # Get current asset list\n",
    "# asset_list = utils.get_ee_assets(tcorr_img_coll_id, shell_flag=True)\n",
    "# logging.debug('Displaying first 10 images in collection')\n",
    "# logging.debug(asset_list[:10])\n",
    "    \n",
    "start_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "for export_dt in utils.date_range(start_dt, end_dt, days=1, skip_leap_days=False):   \n",
    "    logging.info('{}'.format(export_dt.strftime('%Y-%m-%d')))\n",
    "    \n",
    "    if export_dt > datetime.datetime.today():\n",
    "        logging.info('  Unsupported date, skipping')\n",
    "        continue\n",
    "    \n",
    "    task_id = 'tcorr_image_{}_{}'.format(tcorr_img_name, export_dt.strftime('%Y%m%d'))\n",
    "    asset_id = '{}/{}'.format(tcorr_img_coll_id, export_dt.strftime('%Y%m%d'))\n",
    "    logging.debug('  Task ID: {}'.format(task_id))\n",
    "    logging.debug('  Asset ID: {}'.format(asset_id))\n",
    "    \n",
    "    if overwrite_flag:\n",
    "        if task_id in tasks.keys():\n",
    "            logging.info('  Task already submitted, cancelling')\n",
    "            ee.data.cancelTask(tasks[task_id])\n",
    "        # This is intentionally not an \"elif\" so that a task can be\n",
    "        # cancelled and an existing image/file/asset can be removed\n",
    "        # if asset_id in asset_list:\n",
    "        if utils.image_exists(asset_id):\n",
    "            logging.info('  Asset already exists, removing')\n",
    "            ee.data.deleteAsset(asset_id)\n",
    "    else:\n",
    "        if task_id in tasks.keys():\n",
    "            logging.info('  Task already submitted, skipping')\n",
    "            continue\n",
    "        # elif asset_id in asset_list:\n",
    "        elif utils.image_exists(asset_id):\n",
    "            logging.info('  Asset already exists, skipping')\n",
    "            continue\n",
    "\n",
    "    # Build and merge the Landsat collections\n",
    "    l8_coll = ee.ImageCollection('LANDSAT/LC08/C01/T1_RT_TOA')\\\n",
    "        .filterDate(export_dt, export_dt + datetime.timedelta(days=1))\\\n",
    "        .filterBounds(tmax_img.geometry())\\\n",
    "        .filterBounds(export_geom)\\\n",
    "        .filterMetadata('CLOUD_COVER_LAND', 'less_than', max_cloud_cover)\\\n",
    "        .filterMetadata('DATA_TYPE', 'equals', 'L1TP')\n",
    "    l7_coll = ee.ImageCollection('LANDSAT/LE07/C01/T1_RT_TOA')\\\n",
    "        .filterDate(export_dt, export_dt + datetime.timedelta(days=1))\\\n",
    "        .filterBounds(tmax_img.geometry())\\\n",
    "        .filterBounds(export_geom)\\\n",
    "        .filterMetadata('CLOUD_COVER_LAND', 'less_than', max_cloud_cover)\\\n",
    "        .filterMetadata('DATA_TYPE', 'equals', 'L1TP')\n",
    "    landsat_coll = l8_coll.merge(l7_coll)\n",
    "    # l5_coll = ee.ImageCollection('LANDSAT/LT05/C01/T1_TOA')\\\n",
    "    #     .filterDate(export_dt, export_dt + datetime.timedelta(days=1))\\\n",
    "    #     .filterBounds(tmax_img.geometry())\\\n",
    "    #     .filterBounds(export_geom)\\\n",
    "    #     .filterMetadata('CLOUD_COVER_LAND', 'less_than', max_cloud_cover)\\\n",
    "    #     .filterMetadata('DATA_TYPE', 'equals', 'L1TP')\n",
    "    # landsat_coll = l8_coll.merge(l7_coll).merge(l5_coll)\n",
    "    # pprint.pprint(landsat_coll.aggregate_histogram('system:index').getInfo())\n",
    "            \n",
    "    def tcorr_img_func(image):\n",
    "        t_stats = ssebop.Image.from_landsat_c1_toa(ee.Image(image)).tcorr_stats\n",
    "        tcorr = ee.Algorithms.If(t_stats.get('tcorr_p5'), ee.Number(t_stats.get('tcorr_p5')), 0)\n",
    "        # tcorr = ee.Number(t_stats.get('tcorr_p5'))\n",
    "        count = ee.Number(t_stats.get('tcorr_count'))\n",
    "        \n",
    "        # Remove the merged collection indices from the system:index\n",
    "        scene_id = ee.List(ee.String(image.get('system:index')).split('_')).slice(-3)\n",
    "        scene_id = ee.String(scene_id.get(0)).cat('_')\\\n",
    "            .cat(ee.String(scene_id.get(1))).cat('_')\\\n",
    "            .cat(ee.String(scene_id.get(2)))\n",
    "        \n",
    "        return ee.Image([\n",
    "                tmax_img.select([0], ['tcorr']).double().multiply(0).add(ee.Image.constant(tcorr)),\n",
    "                tmax_img.select([0], ['count']).double().multiply(0).add(ee.Image.constant(count))])\\\n",
    "            .clip(image.geometry())\\\n",
    "            .updateMask(1)\\\n",
    "            .setMulti({\n",
    "                # 'system:index': scene_id,\n",
    "                'system:time_start': image.get('system:time_start'),\n",
    "                'SCENE_ID': scene_id,\n",
    "                'WRS2_TILE': scene_id.slice(5, 11),\n",
    "                # 'WRS2_TILE': ee.String('p').cat(scene_id.slice(5, 8)).cat('r').cat(scene_id.slice(8, 11)),\n",
    "                'TCORR': tcorr,\n",
    "                'COUNT': count,\n",
    "        })\n",
    "        #     .copyProperties(image, ['system:time_start', 'system:index'])    \n",
    "    tcorr_img_coll = ee.ImageCollection(landsat_coll.map(tcorr_img_func)) \\\n",
    "        .filterMetadata('COUNT', 'not_less_than', min_pixel_count)\n",
    "    # pprint.pprint(tcorr_img_coll.aggregate_histogram('system:index').getInfo())\n",
    "    # pprint.pprint(ee.Image(tcorr_img_coll.first()).getInfo())\n",
    "\n",
    "    # DEADBEEF - This doesn't work since there seems to be a limit on the type and \n",
    "    #   length of properties for exported assets.\n",
    "    # # Save Tcorr properties for the scene images as a property on the daily image\n",
    "    # def tcorr_ftr_func(tcorr_img):\n",
    "    #     return ee.Feature(\n",
    "    #         None,\n",
    "    #         {\n",
    "    #             'SCENE_ID': ee.String(tcorr_img.get('SCENE_ID')),\n",
    "    #             'TCORR': ee.Number(tcorr_img.get('TCORR')),\n",
    "    #             'COUNT': ee.Number(tcorr_img.get('COUNT')),\n",
    "    #             # 'SSEBOP_VER': ssebop.__version__,\n",
    "    #             # 'TMAX_SOURCE': tmax_source.upper(),\n",
    "    #             # 'TMAX_VERSION': tmax_version.upper(),\n",
    "    #             # 'EXPORT_DATE': datetime.datetime.today().strftime('%Y-%m-%d'),\n",
    "    #             # 'WRS2_TILE': ee.String('p').cat(scene_id.slice(5, 8)).cat('r').cat(scene_id.slice(8, 11)),\n",
    "    #             # 'system:time_start': tcorr_img.get('system:time_start'),\n",
    "    #             # 'system:index': tcorr_img.get('SCENE_ID'),\n",
    "    #         })\n",
    "    \n",
    "    tcorr_img = tcorr_img_coll.mean()\\\n",
    "        .setMulti({\n",
    "            'system:time_start': utils.millis(export_dt),\n",
    "            'WRS2_TILES': ee.String(ee.List(ee.Dictionary(\n",
    "                tcorr_img_coll.aggregate_histogram('WRS2_TILE')).keys()).join(',')),\n",
    "            'SSEBOP_VERSION': ssebop.__version__,\n",
    "            'TMAX_SOURCE': tmax_source.upper(),\n",
    "            'TMAX_VERSION': tmax_version.upper(),\n",
    "            'EXPORT_DATE': datetime.datetime.today().strftime('%Y-%m-%d'),\n",
    "        })\n",
    "    # pprint.pprint(tcorr_img.getInfo())\n",
    "\n",
    "    task = ee.batch.Export.image.toAsset(\n",
    "        image=tcorr_img,\n",
    "        description=task_id,\n",
    "        assetId=asset_id,\n",
    "        crs=export_crs,\n",
    "        crsTransform='[' + ','.join(list(map(str, export_geo))) + ']',\n",
    "        dimensions='{0}x{1}'.format(*export_shape),\n",
    "    )\n",
    "    task.start()\n",
    "    time.sleep(1)\n",
    "    logging.debug('  Status: {}'.format(task.status()['state']))\n",
    "    logging.debug('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcorr_img = ee.Image('{}/{}'.format(tcorr_img_coll_id, '20170701'))\n",
    "# Image(url=ee.Image(tcorr_img).getThumbURL({\n",
    "#     'min': 0.95, 'max': 1.0, 'region': export_region,\n",
    "#     'palette': ['ff0000', 'ffff00', '00ffff', '0000ff']}))\n",
    "# # embed=True, format='png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
